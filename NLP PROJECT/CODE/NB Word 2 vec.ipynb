{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SEWX7cC5AVX32k9PBN-6ZXkSz1eDTV3d","authorship_tag":"ABX9TyN8V77hnlqrGNeZ2QFtutUU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hIa02rptEPZm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","\n","# Load data\n","train_data = pd.read_csv(\"/content/drive/My Drive/NLP PROJECT/Dataset/train.csv\")\n","\n","# Clean text\n","def clean_text(text):\n","    text = re.sub(r'<.*?>', '', text)\n","    emoji_pattern = re.compile(\"[\"\n","                        u\"\\U0001F600-\\U0001F64F\"\n","                        u\"\\U00002702-\\U000027B0\"\n","                        u\"\\U0001F680-\\U0001F6FF\"\n","                        u\"\\U0001F300-\\U0001F5FF\"\n","                        u\"\\U0001F1E0-\\U0001F1FF\"\n","                        u\"\\U000024C2-\\U0001F251\"\n","                        \"]+\")\n","    text = emoji_pattern.sub(r'', text)\n","    text = re.sub(r'@\\S+', ' ', text)\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n","    text = text.lower()\n","    nltk.download('stopwords')\n","    text = [word for word in text.split() if word not in stopwords.words('english')]\n","    nltk.download('wordnet')\n","    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n","    text = ' '.join(word for word in text)\n","    return text\n","\n","# Apply cleaning\n","train_data['clean text'] = train_data['text'].apply(clean_text)\n","\n","# Drop unnecessary columns\n","df_train = train_data.drop(columns=['id', 'keyword', 'location', 'text'])\n","\n","# Split data\n","X = df_train['clean text']\n","y = df_train['target']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Tokenize sentences\n","train_sentences = [sentence.split() for sentence in X_train]\n","test_sentences = [sentence.split() for sentence in X_test]\n","\n","# Train Word2Vec model\n","word2vec_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Function to get sentence vectors\n","def get_sentence_vector(sentence, model, size):\n","    vec = np.zeros(size).reshape((1, size))\n","    count = 0\n","    for word in sentence:\n","        if word in model.wv:\n","            vec += model.wv[word].reshape((1, size))\n","            count += 1\n","    if count != 0:\n","        vec /= count\n","    return vec\n","\n","# Vectorize data\n","word2vec_size = 100\n","train_vectors = np.concatenate([get_sentence_vector(sentence, word2vec_model, word2vec_size) for sentence in train_sentences])\n","test_vectors = np.concatenate([get_sentence_vector(sentence, word2vec_model, word2vec_size) for sentence in test_sentences])\n","\n","# Initialize and train classifier\n","nb_classifier = MultinomialNB()\n","# Note: MultinomialNB requires non-negative input values. Word2Vec vectors might contain negative values.\n","# As a workaround, use MinMaxScaler to scale the data to be non-negative.\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","train_vectors = scaler.fit_transform(train_vectors)\n","test_vectors = scaler.transform(test_vectors)\n","\n","nb_classifier.fit(train_vectors, y_train)\n","\n","# Make predictions and evaluate\n","predictions = nb_classifier.predict(test_vectors)\n","conf_mat = confusion_matrix(y_test, predictions)\n","accuracy = accuracy_score(y_test, predictions)\n","report = classification_report(y_test, predictions)\n","\n","print(\"Confusion Matrix:\\n\", conf_mat)\n","print(\"\\nAccuracy:\", accuracy)\n","print(\"\\nReport:\\n\", report)\n","\n"]}]}